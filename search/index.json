[{"content":"一维卷积 Pytorch的定义 $$\\operatorname{out}(N_i,C_{\\operatorname{out}_j})=\\operatorname{bias}(C_{\\operatorname{out}_j})+\\sum_{k=0}^{C_{in}-1}\\operatorname{weight}(C_{\\operatorname{out}_j},k)\\star\\operatorname{input}(N_i,k)$$ 写成数学形式： $$\ry_{ni}=\\sum_jv_{n(i+j)}w_{nj}\r$$ 偏置被放进求和号作为输入和卷积核的一部分。 将批次忽略，也就是只看某个样本卷积操作的公式就有： $$\ry_i=\\sum_jv_{i+j}w_j\r$$ 定义张量和索引：\n$n$: Batch (批次) 索引 (对应 $N_i$) $j$: 输出通道索引 (对应 $C_{out, j}$) $k$: 输入通道索引 (对应 $k$) $i$: 输出空间索引 (对应 $i_{\\bar{y}}$) $l$: 输入空间索引 (对应 $l_{\\bar{x}}$) $p$: 核 (kernel) 空间索引 (对应 $j_{\\bar{k}}$) 现在定义参与运算的张量：\n输出 $O$: $O_{nji}$ (Batch, Out-Channel, Out-Spatial) 输入 $I$: $I_{nkl}$ (Batch, In-Channel, In-Spatial) 权重 $W$: $W_{jkp}$ (Out-Channel, In-Channel, Kernel-Spatial) 偏差 $B$: $B_{j}$ (Out-Channel) 卷积张量 $C$: $C_{ilp}$ (Out-Spatial, In-Spatial, Kernel-Spatial) 定义卷积张量 $C$ : 这个 $C$ 张量定义了卷积操作本身 $$C_{ijl}=\r\\begin{cases}\r1, \u0026 \\mathrm{if} \\quad l=i+j \\\\\r0, \u0026 \\mathrm{else} \u0026 \\end{cases}$$ 卷积操作的张量计算公式： $$y_i=C_{ijl}v_lw_j$$ 这里的 $v$ 表示输入数据，下标 $l$ 表示 $v$ 的第 $l$ 个分量。$w_{j}$ 表示卷积核的第 $j$ 个分量。$C_{ijl}$ 则表示卷积操作，将约束条件带入上式，就可得到 $$\ry_i=v_{i+j}w_j\r$$ 扩\n加入步幅 $s$ 和膨胀 $d$ ： 此时卷积操作加入了步幅长度 $s$ 和膨胀大小 $d$ ： $$C_{ilp} = \\begin{cases} 1, \u0026 \\text{if } l = s \\cdot i + d \\cdot p \\\\ 0, \u0026 \\text{else} \\end{cases}$$ 让我们通过一个简单的例子（$d=1, s=2$ vs $s=1, d=2$）来理解。 Stride (步幅) 的效果: $s=2, d=1$\n公式变为： $l = 2 \\cdot i + j$ 它在做什么： 缩放输出索引 $i$。索引 $i$ 代表你正在计算第几个输出点。 我们来计算几个输出点： 计算 $y_0$ (即 $i=0$): 关系是 $l = 2 \\cdot 0 + j = j$。 $y_0 = \\sum_j v_{j} \\cdot w_j = v_0w_0 + v_1w_1 + v_2w_2 + \\dots$ （核从输入的第 0 个位置开始） 计算 $y_1$ (即 $i=1$): 关系是 $l = 2 \\cdot 1 + j = 2 + j$。 $y_1 = \\sum_j v_{2+j} \\cdot w_j = v_2w_0 + v_3w_1 + v_4w_2 + \\dots$ （核从输入的第 2 个位置开始） 计算 $y_2$ (即 $i=2$): 关系是 $l = 2 \\cdot 2 + j = 4 + j$。 $y_2 = \\sum_j v_{4+j} \\cdot w_j = v_4w_0 + v_5w_1 + v_6w_2 + \\dots$ （核从输入的第 4 个位置开始） 结论 (Stride): 请注意：从计算 $y_0$ 变到 $y_1$，你的核在输入 $v$ 上滑动（或“跳跃”）了 2 个位置（从 0 到 2）。你跳过了本应在 $s=1$ 时计算的那个输出点。 Stride 导致你计算的输出点 $y_i$ 的总数变少了。这就是缩放（下采样）效果的来源。 Dilation (扩张) 的效果: $s=1, d=2$\n公式变为： $l = i + 2 \\cdot j$ 它在做什么： 缩放核索引 $j$。索引 $j$ 代表核 $w$ 内部的第几个权重。 我们来计算几个输出点： 计算 $y_0$ (即 $i=0$): 关系是 $l = 0 + 2 \\cdot j = 2j$。 $y_0 = \\sum_j v_{2j} \\cdot w_j = v_0w_0 + v_2w_1 + v_4w_2 + \\dots$ 计算 $y_1$ (即 $i=1$): 关系是 $l = 1 + 2 \\cdot j = 1 + 2j$。 $y_1 = \\sum_j v_{1+2j} \\cdot w_j = v_1w_0 + v_3w_1 + v_5w_2 + \\dots$ 计算 $y_2$ (即 $i=2$): 关系是 $l = 2 + 2 \\cdot j = 2 + 2j$。 $y_2 = \\sum_j v_{2+2j} \\cdot w_j = v_2w_0 + v_4w_1 + v_6w_2 + \\dots$ 结论 (Dilation): 请注意：我们没有跳过任何输出点（我们计算了 $y_0, y_1, y_2, \\dots$）。 相反，为了计算单个输出点（比如 $y_0$），你的核 $w$ 的权重 ($w_0, w_1, w_2$) 被应用到了间隔更开的输入点 ($v_0, v_2, v_4$) 上。 Dilation 并没有减少输出点的数量，而是增大了单个核的感受野 (receptive field)。核被‘撑大’了，中间有了‘空洞’。这就是扩张的含义。 二维卷积 为简化表示，我们做如下索引替换：\n输出索引为 $(a, b)$ 核索引为 $(c, d)$ 输入索引为 $(e, f)$ 定义张量： 令输出张量为 $y$，其分量为 $y_{ab}$。 令输入张量为 $v$，其分量为 $v_{ef}$。 令核张量为 $w$，其分量为 $w_{cd}$。 令 $(\\star 2D)$ 卷积张量为 $C$，其分量为 $C_{abcdef}$。 张量缩并（爱因斯坦求和形式）： 完整的 2D 卷积操作（类似于 1D 情况）可以写为： $$y_{ab} = \\sum_{c,d} \\sum_{e,f} C_{abcdef} \\cdot v_{ef} \\cdot w_{cd}$$ 使用爱因斯坦求和约定（省略 $\\sum$ 符号），我们得到一个非常简洁的张量形式： $$y_{ab} = C_{abcdef} v_{ef} w_{cd}$$ $a, b$ 是自由索引，代表输出张量的维度。 $c, d, e, f$ 是哑索引（缩并索引），因为它们在右侧重复出现，表示需要对它们的所有可能值求和。 六阶卷积张量 $C$ 的定义： 这个六阶张量 $C_{abcdef}$ 的定义即是图像中 $(\\star 2D)$ 的定义： $$C_{abcdef} = \\begin{cases} 1, \u0026 \\text{if } (e, f) = (a, b) + (c, d) \\\\ 0, \u0026 \\text{else} \\end{cases}$$ 这等价于两个独立的条件必须同时满足： $e = a + c$ $f = b + d$ 验证 这种形式再次等价于标准的 2D 卷积。如果我们从 $y_{ab} = \\sum_{c,d} \\sum_{e,f} C_{abcdef} v_{ef} w_{cd}$ 开始：\n对 $e, f$ 求和：由于 $C_{abcdef}$ 仅在 $e=a+c$ 和 $f=b+d$ 时为 1，$\\sum_{e,f} C_{abcdef} v_{ef}$ 这一项塌缩为 $v_{a+c, b+d}$。 代回结果：$y_{ab} = \\sum_{c,d} v_{a+c, b+d} \\cdot w_{cd}$。 这正是 2D 卷积的标准定义（将 $(a,b)$ 视为输出坐标，$(c,d)$ 视为核坐标）。 加入步幅和膨胀\n步幅 (Stride): $s = (s_0, s_1)$ 扩张 (Dilation): $d = (d_0, d_1)$ 张量缩并 (爱因斯坦求和形式): 整个 2D 卷积操作（包含通道）可以写为： $$y_{ab} = C_{abcdef} v_{ef} w_{cd}$$ $a, b$ 是自由索引，定义了我们正在计算的输出张量的分量。 $c, d, e, f$ 是哑索引，表示我们需要对这些索引的所有可能值求和。 六阶卷积张量 $C$ 的定义: 这个六阶张量 $C$ 包含了所有关于步幅和扩张的结构信息。其分量 $C_{abcdef}$ 定义如下： $$C_{abcdef} = \\begin{cases} 1, \u0026 \\text{if } (e = s_0 \\cdot a + d_0 \\cdot c) \\quad \\textbf{and} \\quad (f = s_1 \\cdot b + d_1 \\cdot d) \\\\ 0, \u0026 \\text{else} \\end{cases}$$ 这个定义非常清晰地展示了 $s$ 和 $d$ 是如何分别作用于不同维度的： $s_0, d_0$ 掌控着第 0 维 (索引 $a, c, e$) 的映射关系。 $s_1, d_1$ 掌控着第 1 维 (索引 $b, d, f$) 的映射关系。 ","date":"2025-11-17T14:19:08+08:00","permalink":"http://localhost:1313/hugo_dev/p/%E5%8D%B7%E7%A7%AF%E6%A8%A1%E5%9D%97%E7%9A%84%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E5%8F%8A%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/","title":"卷积模块的张量表示及梯度推导"},{"content":"代码部分 这是多层感知机（MLP）的一段示例代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import os import torch from torch import nn class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork().to(device) X = torch.rand(1, 28, 28, device=device) logits = model(X) pred_probab = nn.Softmax(dim=1)(logits) y_pred = pred_probab.argmax(1) 神经网络的结构 nn.Flatten() 这个函数将输入张量的各维度展平成一个向量。默认从索引1开始到索引-1结束，要注意的是索引是从0开始的，所以索引1其实是指的第2个。 这里的输入X维度为$[1,28,28]$，经过nn.Flatten()后应该是$[1,28*28]$。 从张量角度看， $$\rx_{b1ij}=x_{bn}, \\quad n=0*28*28+i*28+j\r$$ linear_relu_stack 这个函数将一系列的神经层排成一个列表，数据将从这些层中流出。过程中会改变维度。\nLinear(28*28, 512) 张量表示: $y_{bm}=x_{bn}w_{nm}+\\mathbf{1}_{b}b_{m}$ ReLU 张量表示: $y_{bm}^{r}=\\text{ReLU}(y_{bm})$ Linear(512,512) 张量表示: $y_{bm'}'=y_{bm}^{r}w_{mm'}+\\mathbf{1}_{b}b_{m'}$ ReLU 张量表示: $y_{bm'}^{r}=\\text{ReLU}(y_{bm'})$ Linear(512,10) 张量表示: $y_{bo}=y_{bm'}^{r}w_{m'o}+\\mathbf{1}_{b}b_{o}$ Softmax(dim=1) 张量表示： $$s_{bi}= \\frac{e^{x_{bi}}}{\\mathbf{1}_{kj}e^{x_{kj}}}$$ 交叉熵损失 $$\rc_{b}=-\\log{s_{b\\text{T}}}\r$$ 交叉熵损失是单分类损失函数，这里的$\\text{T}$是代表真实标签的常量索引，这里的$\\text{T}$不是一种哑指标。\n推导梯度 交叉熵部分梯度 重点是利用反向传播，也就是由终点向起点一层层推。\n$c_{b}$ $$\r\\frac{\\partial c_{b}}{\\partial s_{ki}}=-\\frac{\\partial \\log s_{b\\text{T}}}{\\partial s_{ki}}\r=-\\frac{1}{s_{k\\text{T}}}\\delta_{i\\text{T}}\r$$ $s_{bi}$ $$\r\\frac{\\partial s_{bi}}{\\partial y_{ko}}\r=\\frac{\\partial \\mathbf{1}_{kj}e^{x_{kj}}}{\\partial y_{ko}}\r$$ 令$\\sigma=\\mathbf{1}_{kj}e^{x_{kj}}$, $$\r\\frac{\\partial \\mathbf{1}_{kj}e^{x_{kj}}}{\\partial y_{ko}}\r=\\frac{\\partial\\sigma}{\\partial y_{ko}}\r=\\frac{1}{\\sigma^2}( \\frac{\\partial e^{y_{bi}}}{\\partial y_{ko}}\\sigma-e^{y_{bi}}\\frac{\\partial \\sigma}{\\partial y_{ko}})\r$$ $\\frac{\\partial e^{y_{bi}}}{\\partial y_{ko}}$ $$\\frac{\\partial e^{y_{bi}}}{\\partial y_{ko}}\r=e^{y_{bi}}\\frac{\\partial y_{bi}}{\\partial y_{ko}}\r=e^{y_{bi}}\\delta_{bk}\\delta_{io}$$ $\\frac{\\partial \\sigma}{\\partial y_{ko}}$ $$\\frac{\\partial \\sigma}{\\partial y_{ko}}=\\frac{\\partial \\mathbf{1}_{bj}e^{y_{bj}}}{\\partial y_{ko}}\r=\\mathbf{1}_{bj}e^{y_{bj}}\\delta_{bk}\\delta_{jo}=\\mathbf{1}_{ko}e^{y_{ko}}\r=e^{y_{ko}} $$ 代入原式： $$\r\\frac{1}{\\sigma^2}( \\frac{\\partial e^{y_{bi}}}{\\partial y_{ko}}\\sigma-e^{y_{bi}}\\frac{\\partial \\sigma}{\\partial y_{ko}})\r=\\frac{1}{\\sigma^2}( e^{y_{bi}}\\delta_{bk}\\delta_{io}\\sigma-e^{y_{bi}}e^{y_{ko}})\r=s_{bi}\\delta_{bk}\\delta_{io}-s_{bi}s_{ko}\r$$ 即 $$\\frac{\\partial s_{bi}}{\\partial y_{ko}}\r=s_{bi}\\delta_{bk}\\delta_{io}-s_{bi}s_{ko}$$ 综合一下，现在就有 $$\r\\frac{\\partial c_{b}}{\\partial y_{ko}}\r=\\frac{\\partial c_{b}}{\\partial s_{ki}}\\frac{\\partial s_{bi}}{\\partial y_{ko}}\r=-\\frac{1}{s_{k\\text{T}}}\\delta_{i\\text{T}}(s_{bi}\\delta_{bk}\\delta_{io}-s_{bi}s_{ko})\r=s_{ko}-\\delta_{bk}\\delta_{To}\r$$ 这里可以看出，交叉熵损失在最后一层的梯度，恰好等于第$k$个样本的第$o$个logits(概率值)减去第$k$个样本的第$\\text{T}$个标签(也就是1). 单个线性层梯度 输入 $X$: (∗, in_features) 权重 $A$ : (out_features,in_features) 偏置 $b$: (out_features) 输出 $Y$: (∗, out_features) PyTorch对线性层的定义： $$\rY=XA^{T}+b\r$$ 令 $C=A^{T}$ ，就有张量表示： $$\ry_{ij}=x_{ik}c_{kj}+\\mathbf{1}_{i}b_{j}\r$$ 上式中，$k$ 重复出现代表做了乘积，同时在等式左侧未出现，就还做了求和。$\\mathbf{1}_{i}$ 用于将 $b_{j}$ 广播成矩阵。\n根据# Linear Layer, Deriving the Gradient for the Backward Pass的结果，我们有如下结论：\n对 $X$ 的梯度 $$\\frac{\\partial y_{ij}}{\\partial x_{pq}}=\\delta_{ip}c_{qj}$$ $$\\begin{aligned}\r\\frac{\\partial l}{\\partial x_{pq}} \u0026 =\\frac{\\partial l}{\\partial y_{ij}}\\frac{\\partial y_{ij}}{\\partial x_{pq}} \\\\\r\u0026 =\\frac{\\partial l}{\\partial y_{ij}}\\delta_{ip}c_{qj} \\\\\r\u0026 =\\frac{\\partial l}{\\partial y_{pj}}c_{qj}\r\\end{aligned}$$ 2. 对 $A$ 的梯度 $$\\begin{aligned}\r\\frac{\\partial y_{ij}}{\\partial a_{pq}} \u0026 =\\frac{\\partial}{\\partial a_{pq}}[x_{ik}c_{kj}+\\mathbf{1}_ib_j] \\\\\r\u0026 =x_{ik}\\frac{\\partial c_{kj}}{\\partial a_{pq}}\r\\end{aligned}$$ 由于 $C=A^{T}$， 所以有 $c_{kj}=a_{jk}$ 。继续有公式： $$\\begin{aligned}\r\\frac{\\partial y_{ij}}{\\partial a_{pq}} \u0026 =x_{ik}\\frac{\\partial a_{jk}}{\\partial a_{pq}} \\\\\r\u0026 =x_{ik}\\delta_{jp}\\delta_{kq} \\\\\r\u0026 =x_{iq}\\delta_{jp}\r\\end{aligned}$$ $$\\begin{aligned}\r\\frac{\\partial l}{\\partial a_{pq}} \u0026 =\\frac{\\partial l}{\\partial y_{ij}}\\frac{\\partial y_{ij}}{\\partial a_{pq}} \\\\\r\u0026 =\\frac{\\partial l}{\\partial y_{ij}}x_{iq}\\delta_{jp} \\\\\r\u0026 =\\frac{\\partial l}{\\partial y_{ip}}x_{iq}\r\\end{aligned}$$ 对 $b$ 的梯度 $$\\begin{aligned}\r\\frac{\\partial y_{ij}}{\\partial b_p} \u0026 =\\frac{\\partial}{\\partial b_p}[x_{ik}c_{kj}+\\mathbf{1}_ib_j] \\\\\r\u0026 =\\frac{\\partial\\left(\\mathbf{1}_ib_j\\right)}{\\partial b_p} \\\\\r\u0026 =1_i\\frac{\\partial b_j}{\\partial b_p}\r\\end{aligned}$$ $$\\frac{\\partial y_{ij}}{\\partial b_p}=\\mathbf{1}_i\\delta_{jp}$$ $$\\begin{aligned}\r\\frac{\\partial l}{\\partial b_p} \u0026 =\\frac{\\partial l}{\\partial y_{ij}}\\frac{\\partial y_{ij}}{\\partial b_p} \\\\\r\u0026 =\\frac{\\partial l}{\\partial y_{ij}}\\mathbf{1}_i\\delta_{jp} \\\\\r\u0026 =\\frac{\\partial l}{\\partial y_{ip}}\\mathbf{1}_i\r\\end{aligned}$$ ","date":"2025-11-17T01:43:46+08:00","permalink":"http://localhost:1313/hugo_dev/p/mlp%E7%9A%84%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E5%8F%8A%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/","title":"MLP的张量表示及梯度推导"},{"content":"在探索使用hugo建立博客的过程中，遇到了数学公式难以渲染的问题。\n如何配置hugo的数学环境\n如何在 Hugo 中使用数学公式？\ntips: KaTex 中的行内公式也是支持使用$inline_equation$的\nMarkdown下的Tex渲染失败解决\nMarkdown下的Tex渲染失败解决\nhugo网页渲染是网站和公式两个渲染引擎同时作用的，所以要设置优先级，让公式优先被公式引擎渲染。\nKaTex常用数学公式\n常用数学公式排版KaTex语法总结\n修改完之后需要多等待几秒，网页刷新较慢。多在本地提交修改，最终再上传至仓库。\n","date":"2025-11-17T01:12:58+08:00","permalink":"http://localhost:1313/hugo_dev/p/hugo%E5%8D%9A%E5%AE%A2%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/","title":"Hugo博客的数学公式问题"},{"content":"自注意力 代码部分\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch import torch.nn as nn class SelfAttention(nn.Module): def __init__(self, d_in, d_out, qkv_bias=False): self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) def forward(self, x): q = self.W_q(x) k = self.W_k(x) v = self.W_v(x) attn_scores = torch.matmul(q, k.T) attn_weights = torch.softmax(attn_scores / k.shape[-1] ** 0.5, dim=-1) context_vecs = torch.matmul(attn_weights, v) return context_vecs 前向过程 索引和张量定义\n$t_1, t_2$: Token/序列 (Sequence) 索引。$t_1$ 代表查询 (Query)，$t_2$ 代表键/值 (Key/Value)。 $i, k$: 输入特征维度 ($d\\_in$) 的索引。 $j$: 输出/内部特征维度 ($d\\_out$) 的索引。 输入张量： Input $X$: $X_{ti}$ (对应代码中的 x) Query Weights $W_Q$: $W^Q_{ji}$ (对应 self.W_q.weight) Key Weights $W_K$: $W^K_{jk}$ (对应 self.W_k.weight) Value Weights $W_V$: $W^V_{jk}$ (对应 self.W_v.weight) Output $Y$: $Y_{t_1 j}$ (对应 context_vecs) q, k, v 的计算 (线性) 这是输入 $X$ 和权重矩阵之间的张量缩并（矩阵乘法）。 Query: $Q_{t_1 j} = X_{t_1 i} W^Q_{ji}$ Key: $K_{t_2 j} = X_{t_2 k} W^K_{jk}$ Value: $V_{t_2 j} = X_{t_2 k} W^V_{jk}$ （在爱因斯坦约定中，重复的索引 $i$ 和 $k$ 表示在 $d\\_in$ 维度上求和。） attn_scores 的计算 (线性) attn_scores = torch.matmul(q, k.T)。这是一个矩阵乘法，它在 $j$ (即 $d\\_out$) 维度上进行缩并。 $$S_{t_1 t_2} = Q_{t_1 j} K_{t_2 j}$$ attn_weights 的计算 (非线性) attn_weights = torch.softmax(...)。这一步是非线性的，不能用简单的张量缩并表示。我们将其定义为函数 $A = \\text{softmax}(S')$： $$S'_{t_1 t_2} = S_{t_1 t_2} / \\sqrt{d_{out}}$$ $$A_{t_1 t_2} = \\frac{\\exp(S'_{t_1 t_2})}{\\mathbf{1}_{t_2'} \\exp(S'_{t_1 t_2'})}$$ $A_{t_1 t_2}$ 是最终的注意力权重矩阵。 context_vecs 的计算 (线性) context_vecs = torch.matmul(attn_weights, v)。这是注意力权重 $A$ 和值 $V$ 之间的矩阵乘法。它在 $t_2$ 维度（序列）上进行缩并： $$Y_{t_1 j} = A_{t_1 t_2} V_{t_2 j}$$ 总结：组合的张量形式 我们可以把这些步骤代入，得到一个更完整的（但分步的）张量表示：\n计算分数 (Scores): $$S_{t_1 t_2} = (X_{t_1 i} W^Q_{ji}) (X_{t_2 k} W^K_{jk})$$ 应用非线性 (Softmax): $$A_{t_1 t_2} = \\text{softmax}\\left( \\frac{S_{t_1 t_2}}{\\sqrt{d_{out}}} \\right)$$ 计算上下文 (Context): $$Y_{t_1 j} = A_{t_1 t_2} (X_{t_2 k} W^V_{jk})$$ 最终的输出 $Y_{t_1 j}$ 是所有 $t_2$ 位置的值（经过 $W^V$ 变换后）的动态加权和，权重 $A_{t_1 t_2}$ 是根据查询和键（$X$ 和 $W^Q, W^K$）动态计算出来的。\n梯度推导 引言和符号定义 我们假设我们已经拥有了损失 $L$ 相对于注意力层最终输出 $Y$ 的梯度。我们将其定义为 $G^Y$：\n$$G^Y_{t_1 j} = \\frac{\\partial L}{\\partial Y_{t_1 j}}$$ 目标：\n我们希望求解以下三个梯度：\n$\\frac{\\partial L}{\\partial W^V_{jk}}$ (Value 权重)\n$\\frac{\\partial L}{\\partial W^Q_{ji}}$ (Query 权重)\n$\\frac{\\partial L}{\\partial W^K_{jk}}$ (Key 权重)\n中间梯度符号：\n为了使推导更清晰，我们定义 $G^Z = \\frac{\\partial L}{\\partial Z}$，表示 $L$ 对任意中间张量 $Z$ 的梯度。\n第 1 部分：$W^V$ (Value) 的梯度推导 此路径不经过 Softmax，最为直接。\n反向传播路径是：$L \\rightarrow Y \\rightarrow V \\rightarrow W^V$。\n步骤 1.1：计算 $\\frac{\\partial L}{\\partial V}$ (即 $G^V$)\n前向方程： $Y_{t_1 j} = A_{t_1 t_2} V_{t_2 j}$\n应用链式法则： 我们对一个特定的 $V_{t_2 j}$ 求导：\n$$G^V_{t_2 j} = \\frac{\\partial L}{\\partial V_{t_2 j}} = \\sum_{t_1'} \\sum_{j'} \\frac{\\partial L}{\\partial Y_{t_1' j'}} \\frac{\\partial Y_{t_1' j'}}{\\partial V_{t_2 j}}$$ 计算局部梯度： $\\frac{\\partial Y_{t_1' j'}}{\\partial V_{t_2 j}} = \\frac{\\partial (A_{t_1' t_2'} V_{t_2' j'})}{\\partial V_{t_2 j}} = A_{t_1' t_2} \\delta_{j' j}$\n代回（并使用爱因斯坦约定）：\n$$G^V_{t_2 j} = G^Y_{t_1 j} A_{t_1 t_2}$$ 步骤 1.2：计算 $\\frac{\\partial L}{\\partial W^V}$\n前向方程： $V_{t_2 j} = X_{t_2 k} W^V_{jk}$\n应用链式法则：\n$$\\frac{\\partial L}{\\partial W^V_{jk}} = \\sum_{t_2'} \\sum_{j'} \\frac{\\partial L}{\\partial V_{t_2' j'}} \\frac{\\partial V_{t_2' j'}}{\\partial W^V_{jk}}$$ 计算局部梯度： $\\frac{\\partial V_{t_2' j'}}{\\partial W^V_{jk}} = \\frac{\\partial (X_{t_2' k'} W^V_{j' k'})}{\\partial W^V_{jk}} = X_{t_2' k} \\delta_{j' j}$\n代回（并使用爱因斯坦约定）：\n$$\\frac{\\partial L}{\\partial W^V_{jk}} = G^V_{t_2 j} X_{t_2 k}$$ $W^V$ 梯度总结 将步骤 1.1 代入 1.2，得到 $W^V$ 的最终梯度：\n$$\\frac{\\partial L}{\\partial W^V_{jk}} = (G^Y_{t_1 j} A_{t_1 t_2}) X_{t_2 k}$$ 第 2 部分：$W^Q$ (Query) 和 $W^K$ (Key) 的梯度推导 此路径是 $L \\rightarrow Y \\rightarrow A \\rightarrow S' \\rightarrow S \\rightarrow \\{Q, K\\} \\rightarrow \\{W^Q, W^K\\}$。\n步骤 2.1：计算 $\\frac{\\partial L}{\\partial A}$ (即 $G^A$)\n前向方程： $Y_{t_1 j} = A_{t_1 t_2} V_{t_2 j}$\n应用链式法则：\n$$G^A_{t_1 t_2} = \\frac{\\partial L}{\\partial A_{t_1 t_2}} = \\sum_{t_1'} \\sum_{j'} \\frac{\\partial L}{\\partial Y_{t_1' j'}} \\frac{\\partial Y_{t_1' j'}}{\\partial A_{t_1 t_2}}$$ 计算局部梯度： $\\frac{\\partial Y_{t_1' j'}}{\\partial A_{t_1 t_2}} = \\frac{\\partial (A_{t_1' t_2'} V_{t_2' j'})}{\\partial A_{t_1 t_2}} = \\delta_{t_1' t_1} V_{t_2 j'}$\n代回（并使用爱因斯坦约定）：\n$$G^A_{t_1 t_2} = G^Y_{t_1 j} V_{t_2 j}$$ 步骤 2.2：计算 $\\frac{\\partial L}{\\partial S}$ (即 $G^S$)\n这是最关键的步骤：$L \\rightarrow A \\rightarrow S' \\rightarrow S$。\n前向方程：\n$S'_{t_1 t_2} = S_{t_1 t_2} / \\sqrt{d_{out}}$\n$A_{t_1 t_2} = \\frac{\\exp(S'_{t_1 t_2})}{\\sum_{t_2''} \\exp(S'_{t_1 t_2''})}$ (Softmax)\n应用链式法则：\n我们想求$\\frac{\\partial L}{\\partial S_{t_1 t_2}}$(一个特定的$S$元素)。$S_{t_1 t_2}$通过$S_{t_1 t_2}'$影响第$t_1$行的所有$A$元素（即$A_{t_1 t_2'}$，其中$t_2'$是该行的任一列）。\n$$G^S_{t_1 t_2} = \\frac{\\partial L}{\\partial S_{t_1 t_2}} = \\sum_{t_2'} \\frac{\\partial L}{\\partial A_{t_1 t_2'}} \\frac{\\partial A_{t_1 t_2'}}{\\partial S_{t_1 t_2}}$$ (注意：Softmax 是逐行应用的，所以 $S_{t_1 t_2}$ 只影响 $A_{t_1 :}$，这就是为什么求和中没有 $t_1'$。)\n计算局部梯度（$\\frac{\\partial A_{t_1 t_2'}}{\\partial S_{t_1 t_2}}$）：\n我们需要再次使用链式法则： $$\r\\frac{\\partial A_{t_1 t_2'}}{\\partial S_{t_1 t_2}} = \\sum_{t_2''} \\frac{\\partial A_{t_1 t_2'}}{\\partial S'_{t_1 t_2''}}\\frac{\\partial S'_{t_1 t_2''}}{\\partial S_{t_1 t_2}}\r$$ 缩放梯度： $\\frac{\\partial S'_{t_1 t_2''}}{\\partial S_{t_1 t_2}} = \\frac{\\partial (S_{t_1 t_2''} / \\sqrt{d_{out}})}{\\partial S_{t_1 t_2}} = \\frac{1}{\\sqrt{d_{out}}} \\delta_{t_2'' t_2}$\nSoftmax 雅可比矩阵： $\\frac{\\partial A_{t_1 t_2'}}{\\partial S'_{t_1 t_2''}}$\nCase 1: $t_2' = t_2''$ (对角线元素)\n$\\frac{\\partial A_{t_1 t_2'}}{\\partial S'_{t_1 t_2'}} = A_{t_1 t_2'} (1 - A_{t_1 t_2'})$\nCase 2: $t_2' \\neq t_2''$ (非对角线元素)\n$\\frac{\\partial A_{t_1 t_2'}}{\\partial S'_{t_1 t_2''}} = -A_{t_1 t_2'} A_{t_1 t_2''}$\n组合 (Kronecker Delta): $\\frac{\\partial A_{t_1 t_2'}}{\\partial S'_{t_1 t_2''}} = A_{t_1 t_2'} (\\delta_{t_2' t_2''} - A_{t_1 t_2''})$\n代回 (1) 和 (2) 到局部梯度：\n$$\\frac{\\partial A_{t_1 t_2'}}{\\partial S_{t_1 t_2}} = \\sum_{t_2''} \\left[ A_{t_1 t_2'} (\\delta_{t_2' t_2''} - A_{t_1 t_2''}) \\right] \\left[ \\frac{1}{\\sqrt{d_{out}}} \\delta_{t_2'' t_2} \\right]$$ 由于 $\\delta_{t_2'' t_2}$， $t_2''$ 上的求和坍缩，我们只需令 $t_2'' = t_2$:\n$$\\frac{\\partial A_{t_1 t_2'}}{\\partial S_{t_1 t_2}} = \\frac{1}{\\sqrt{d_{out}}} A_{t_1 t_2'} (\\delta_{t_2' t_2} - A_{t_1 t_2})$$ 代回 $G^S$ 的推导：\n$$G^S_{t_1 t_2} = \\sum_{t_2'} G^A_{t_1 t_2'} \\left[ \\frac{1}{\\sqrt{d_{out}}} A_{t_1 t_2'} (\\delta_{t_2' t_2} - A_{t_1 t_2}) \\right]$$ $$G^S_{t_1 t_2} = \\frac{1}{\\sqrt{d_{out}}} \\sum_{t_2'} \\left( G^A_{t_1 t_2'} A_{t_1 t_2'} \\delta_{t_2' t_2} - G^A_{t_1 t_2'} A_{t_1 t_2'} A_{t_1 t_2} \\right)$$ 拆分求和：\n$\\sum_{t_2'} G^A_{t_1 t_2'} A_{t_1 t_2'} \\delta_{t_2' t_2}$ = (由于 $\\delta_{t_2' t_2}$， $t_2'$ 上的求和坍缩) = $G^A_{t_1 t_2} A_{t_1 t_2}$\n$\\sum_{t_2'} G^A_{t_1 t_2'} A_{t_1 t_2'} A_{t_1 t_2} = A_{t_1 t_2} \\sum_{t_2'} G^A_{t_1 t_2'} A_{t_1 t_2'}$\n组合：\n$$G^S_{t_1 t_2} = \\frac{1}{\\sqrt{d_{out}}} \\left( G^A_{t_1 t_2} A_{t_1 t_2} - A_{t_1 t_2} \\sum_{t_2'} G^A_{t_1 t_2'} A_{t_1 t_2'} \\right)$$ $$G^S_{t_1 t_2} = \\frac{A_{t_1 t_2}}{\\sqrt{d_{out}}} \\left( G^A_{t_1 t_2} - \\sum_{t_2'} G^A_{t_1 t_2'} A_{t_1 t_2'} \\right)$$ 用爱因斯坦约定表示：\n$$G^S_{t_1 t_2} = \\frac{A_{t_1 t_2}}{\\sqrt{d_{out}}} (G^A_{t_1 t_2} - G^A_{t_1 t_2'} A_{t_1 t_2'})$$ (其中 $t_2'$ 是在 $G^A$ 和 $A$ 的逐元素乘积上求和的哑索引。)\n步骤 2.3：计算 $\\frac{\\partial L}{\\partial Q}$ (即 $G^Q$)\n前向方程： $S_{t_1 t_2} = Q_{t_1 j} K_{t_2 j}$\n应用链式法则： (现在我们使用 $G^S$ 作为上游梯度)\n$$G^Q_{t_1 j} = \\frac{\\partial L}{\\partial Q_{t_1 j}} = \\sum_{t_1'} \\sum_{t_2'} \\frac{\\partial L}{\\partial S_{t_1' t_2'}} \\frac{\\partial S_{t_1' t_2'}}{\\partial Q_{t_1 j}}$$ 计算局部梯度： $\\frac{\\partial S_{t_1' t_2'}}{\\partial Q_{t_1 j}} = \\frac{\\partial (Q_{t_1' j'} K_{t_2' j'})}{\\partial Q_{t_1 j}} = \\delta_{t_1' t_1} K_{t_2' j}$\n代回（并使用爱因斯坦约定）：\n$$G^Q_{t_1 j} = G^S_{t_1 t_2} K_{t_2 j}$$ 步骤 2.4：计算 $\\frac{\\partial L}{\\partial K}$ (即 $G^K$)\n前向方程： $S_{t_1 t_2} = Q_{t_1 j} K_{t_2 j}$\n应用链式法则：\n$$G^K_{t_2 j} = \\frac{\\partial L}{\\partial K_{t_2 j}} = \\sum_{t_1'} \\sum_{t_2'} \\frac{\\partial L}{\\partial S_{t_1' t_2'}} \\frac{\\partial S_{t_1' t_2'}}{\\partial K_{t_2 j}}$$ 计算局部梯度： $\\frac{\\partial S_{t_1' t_2'}}{\\partial K_{t_2 j}} = \\frac{\\partial (Q_{t_1' j'} K_{t_2' j'})}{\\partial K_{t_2 j}} = Q_{t_1' j'} \\delta_{t_2' t_2}$\n代回（并使用爱因斯坦约定）：\n$$G^K_{t_2 j} = G^S_{t_1 t_2} Q_{t_1 j}$$ 步骤 2.5：计算 $\\frac{\\partial L}{\\partial W^Q}$\n前向方程： $Q_{t_1 j} = X_{t_1 i} W^Q_{ji}$\n应用链式法则：\n$$\\frac{\\partial L}{\\partial W^Q_{ji}} = \\sum_{t_1'} \\sum_{j'} \\frac{\\partial L}{\\partial Q_{t_1' j'}} \\frac{\\partial Q_{t_1' j'}}{\\partial W^Q_{ji}}$$ 计算局部梯度： $\\frac{\\partial Q_{t_1' j'}}{\\partial W^Q_{ji}} = \\frac{\\partial (X_{t_1' i'} W^Q_{j' i'})}{\\partial W^Q_{ji}} = X_{t_1' i} \\delta_{j' j}$\n代回（并使用爱因斯坦约定）：\n$$\\frac{\\partial L}{\\partial W^Q_{ji}} = G^Q_{t_1 j} X_{t_1 i}$$ 步骤 2.6：计算 $\\frac{\\partial L}{\\partial W^K}$\n前向方程： $K_{t_2 j} = X_{t_2 k} W^K_{jk}$\n应用链式法则：\n$$\\frac{\\partial L}{\\partial W^K_{jk}} = \\sum_{t_2'} \\sum_{j'} \\frac{\\partial L}{\\partial K_{t_2' j'}} \\frac{\\partial K_{t_2' j'}}{\\partial W^K_{jk}}$$ 计算局部梯度： $\\frac{\\partial K_{t_2' j'}}{\\partial W^K_{jk}} = \\frac{\\partial (X_{t_2' k'} W^K_{j' k'})}{\\partial W^K_{jk}} = X_{t_2' k} \\delta_{j' j}$\n代回（并使用爱因斯坦约定）：\n$$\\frac{\\partial L}{\\partial W^K_{jk}} = G^K_{t_2 j} X_{t_2 k}$$ $W^Q$ 和 $W^K$ 梯度总结\n将 2.3 代入 2.5，得到 $W^Q$ 的最终梯度：\n$$\\frac{\\partial L}{\\partial W^Q_{ji}} = (G^S_{t_1 t_2} K_{t_2 j}) X_{t_1 i}$$ 将 2.4 代入 2.6，得到 $W^K$ 的最终梯度：\n$$\\frac{\\partial L}{\\partial W^K_{jk}} = (G^S_{t_1 t_2} Q_{t_1 j}) X_{t_2 k}$$ 其中 $G^S_{t_1 t_2}$ 由步骤 2.2 定义。\n最终梯度总结 以下是损失 $L$ 对三个权重矩阵的完整梯度，其中 $G^S$ 已被明确推导。\nValue 权重梯度 ($W^V$):\n$$\\frac{\\partial L}{\\partial W^V_{jk}} = \\left( \\frac{\\partial L}{\\partial Y_{t_1 j}} A_{t_1 t_2} \\right) X_{t_2 k}$$ 公式解释：Value 权重梯度是误差信号和注意力权重在 $t_{1}$ 上的内积，也就是计算Value在查询token维度上对误差信号有多大贡献，即分配给Value的误差。然后再和输入嵌入在 $t_{2}$ 上的内积，计算此时被 $X_{t_{2}k}$ 激活的信号。\nQuery 权重梯度 ($W^Q$):\n$$\\frac{\\partial L}{\\partial W^Q_{ji}} = \\left( \\frac{\\partial L}{\\partial S_{t_1 t_2}} K_{t_2 j} \\right) X_{t_1 i}$$ 公式解释：\nKey 权重梯度 ($W^K$):\n$$\\frac{\\partial L}{\\partial W^K_{jk}} = \\left( \\frac{\\partial L}{\\partial S_{t_1 t_2}} Q_{t_1 j} \\right) X_{t_2 k}$$ 其中，关键的中间梯度 $\\frac{\\partial L}{\\partial S_{t_1 t_2}}$ (即 $G^S$) 定义为：\n$$\\frac{\\partial L}{\\partial S_{t_1 t_2}} = \\frac{A_{t_1 t_2}}{\\sqrt{d_{out}}} \\left( \\frac{\\partial L}{\\partial A_{t_1 t_2}} - \\sum_{t_2'} \\frac{\\partial L}{\\partial A_{t_1 t_2'}} A_{t_1 t_2'} \\right)$$ 公式解释: 注意力权重对损失的贡献既要看直接贡献也要看平均贡献。括号内的求和实际上就是求平均贡献，是一个相对标准，它奖励那些表现优于“平均水平”的得分，并惩罚那些表现不如平均水平的得分。\n而 $\\frac{\\partial L}{\\partial A_{t_1 t_2}}$ (即 $G^A$) 定义为：\n$$\\frac{\\partial L}{\\partial A_{t_1 t_2}} = \\frac{\\partial L}{\\partial Y_{t_1 j}} V_{t_2 j}$$ 代码验证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 import torch import torch.nn as nn import torch.testing # 导入测试库 # --------------------------------------------------------------------------- # 步骤 1: 修改 SelfAttention 以存储 q, k, v # --------------------------------------------------------------------------- # 我们必须存储 q, k, v 才能在之后访问它们来计算梯度 # class SelfAttention(nn.Module): def __init__(self, d_in, d_out, qkv_bias=False): super().__init__() self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) # 为所有中间张量添加占位符 self.q = None self.k = None self.v = None self.attn_scores = None # S self.attn_weights = None # A self.context_vecs = None # Y def forward(self, x): # 计算 q, k, v 并 *保存* 它们 self.q = self.W_q(x) self.k = self.W_k(x) self.v = self.W_v(x) # d_k (d_out) 的缩放因子 scale = self.k.shape[-1] ** 0.5 # S = Q @ K.T self.attn_scores = torch.matmul(self.q, self.k.transpose(-2, -1)) # A = softmax(S / scale) self.attn_weights = torch.softmax(self.attn_scores / scale, dim=-1) # Y = A @ V self.context_vecs = torch.matmul(self.attn_weights, self.v) return self.context_vecs # --------------------------------------------------------------------------- # 步骤 2: 设置并运行前向和后向传播 # --------------------------------------------------------------------------- # 设置输入维度和输出维度 i,j=10,20 # 创建SelfAttention对象 attention = SelfAttention(d_in=i, d_out=j) # 为了可复现性，固定随机种子 torch.manual_seed(42) # 创建输入张量和上游梯度 x = torch.rand((10, 10, 10), requires_grad=True) # (B, S_in, d_in) y_grad = torch.rand((10, 10, 20)) # (B, S_out, d_out) # 运行前向和后向过程 # 这会为 W_v.weight, W_q.weight, W_k.weight 自动填充 .grad 属性 y = attention(x) y.backward(y_grad) # --------------------------------------------------------------------------- # 步骤 3: 按照你提供的公式手动计算梯度并进行验证 # --------------------------------------------------------------------------- print(\u0026#34;--- 梯度验证开始 ---\u0026#34;) # 索引约定: # b: batch (10) # q: query token 序列 (10) # v: key/value token 序列 (10) # i: d_in (10) # o: d_out (20) # --------------------------------- # 3.A: 计算中间梯度 G^A 和 G^S # --------------------------------- # G^A = dL/dA # 公式: dL/dA[q,v] = dL/dY[q,o] * V[v,o] # einsum: (bqo, bvo) -\u0026gt; bqv grad_A = torch.einsum(\u0026#39;bqo, bvo -\u0026gt; bqv\u0026#39;, y_grad, attention.v) # G^S = dL/dS # 这是 softmax 的反向传播 # 公式: dL/dS = (A / scale) * (dL/dA - sum(dL/dA * A)) scale = attention.k.shape[-1] ** 0.5 A = attention.attn_weights # sum(dL/dA * A) # einsum: (bqv, bqv) -\u0026gt; bq row_dot_sum = torch.einsum(\u0026#39;bqv, bqv -\u0026gt; bq\u0026#39;, grad_A, A).unsqueeze(-1) # 形状变为 (b,q,1) # dL/dS\u0026#39; = A * (dL/dA - sum(dL/dA * A)) # (bqv) * ((bqv) - (bq,1)) -\u0026gt; (bqv) grad_S_prime = A * (grad_A - row_dot_sum) # dL/dS = dL/dS\u0026#39; / scale # (bqv) grad_S = grad_S_prime / scale # --------------------------------- # 3.B: 验证 Value 权重梯度 (W^V) # --------------------------------- try: # 公式: dL/dW_v[o,i] = (dL/dY[q,o] * A[q,v]) * X[v,i] # (注意你的公式中 j-\u0026gt;o, k-\u0026gt;i, t1-\u0026gt;q, t2-\u0026gt;v) # einsum: (bqo, bqv, bvi) -\u0026gt; oi grad_Wv_manual = torch.einsum(\u0026#39;bqo, bqv, bvi -\u0026gt; oi\u0026#39;, y_grad, attention.attn_weights, x) torch.testing.assert_close(grad_Wv_manual, attention.W_v.weight.grad) print(\u0026#34;W_v 梯度计算正确\u0026#34;) except AssertionError as e: print(f\u0026#34;W_v 梯度计算错误:\\n{e}\u0026#34;) # --------------------------------- # 3.C: 验证 Query 权重梯度 (W^Q) # --------------------------------- try: # 公式: dL/dW_q[o,i] = (dL/dS[q,v] * K[v,o]) * X[q,i] # (注意你的公式中 j-\u0026gt;o, i-\u0026gt;i, t1-\u0026gt;q, t2-\u0026gt;v) # einsum: (bqv, bvo, bqi) -\u0026gt; oi grad_Wq_manual = torch.einsum(\u0026#39;bqv, bvo, bqi -\u0026gt; oi\u0026#39;, grad_S, attention.k, x) torch.testing.assert_close(grad_Wq_manual, attention.W_q.weight.grad) print(\u0026#34;W_q 梯度计算正确\u0026#34;) except AssertionError as e: print(f\u0026#34;W_q 梯度计算错误:\\n{e}\u0026#34;) # --------------------------------- # 3.D: 验证 Key 权重梯度 (W^K) # --------------------------------- try: # 公式: dL/dW_k[o,i] = (dL/dS[q,v] * Q[q,o]) * X[v,i] # (注意你的公式中 j-\u0026gt;o, k-\u0026gt;i, t1-\u0026gt;q, t2-\u0026gt;v) # einsum: (bqv, bqo, bvi) -\u0026gt; oi grad_Wk_manual = torch.einsum(\u0026#39;bqv, bqo, bvi -\u0026gt; oi\u0026#39;, grad_S, attention.q, x) torch.testing.assert_close(grad_Wk_manual, attention.W_k.weight.grad) print(\u0026#34;W_k 梯度计算正确\u0026#34;) except AssertionError as e: print(f\u0026#34;W_k 梯度计算错误:\\n{e}\u0026#34;) print(\u0026#34;--- 梯度验证完成 ---\u0026#34;) ","date":"2025-11-16T23:53:52+08:00","permalink":"http://localhost:1313/hugo_dev/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97%E7%9A%84%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E5%8F%8A%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/","title":"注意力模块的张量表示及梯度推导"},{"content":"引言 超越线性模型的一种方式，就是通过增强输入 $X$ 。为 $X$ 赋予非线性，利用 $h_{m}(X)$ 张成的线性空间来讨论非线性问题，我的评价是：用非线性的砖垒一个线性的房子，来模仿非线性。（以曲代直）\n“分段多项式”和“样条”用于局部多项式表示；“小波”基，用于建模信号和图像。 各式各样的基滥用会过拟合，控制复杂度的方式有三种：\n约束方法：可加性模型通过拟合各个部分函数 $f_j$ 的基函数数量$M_j$来控制复杂度（p个函数 $f_j$ ，每个函数对应 $M_j$ 个函数） 选择方法：选择对拟合最有效的基函数来建模（可使用那些变量选择方案） 正则化方法：岭回归、Lasso（既是正则，又是选择） 分段多项式与样条 分段常数是1阶样条，连续分段线性函数是2阶样条。以下是阶段幂基函数的广义形式： $$\rh_{j}^{}\\left( X \\right) =X_{}^{j-1},\\ j=1,...,M, \\\\\rh_{M+l}\\left( X \\right) =\\left( X-\\xi _l \\right) _{+}^{M-1},\\ l=1,...,K.\r$$ 上式中的 $h_j(X)$ 是起始点的各阶基函数， $h_{M+l}(X)$ 是分段后的基函数。\n于是原函数可以被这些基函数线性表示，基的数量计算：\n基函数个数 = (区域数) * (每区域参数个数) - (分段点个数) * (每个分段点的约束数量)，即：\n$$\r\\left( K+1 \\right) \\times \\left( M \\right) -\\left( K \\right) \\times \\left( M-1 \\right) $$ 这些固定节点的样条都称为回归样条，这些方法需要选择样条的阶数、节点的个数和位置。\n正则化和再生核希尔伯特空间（RKHS） 广义的正则化问题，即使是在无穷维函数空间上，它的解依然是有限维的。 基于正定核的问题，相关的函数空间就是RKHS\n方法关系 多项式样条在结点变大时导致严重的舍入问题，此时可以用B-样条（B-Spline），它在结点K变大的同时保持了高效计算效率。 多项式样条在边界上不稳定导致了外推功能较差，此时可以使用自然立方样条（Natural Cubic Spline）\n多维样条分为张量积样条和方差分析样条分解。前者在特征维度较大时，计算量指数上升；后者通过指定交互阶数，将 $h_m(x)h_n(y)$ 变为 $h(x,y)$ ，减少了计算量。 多维样条是平滑样条的多维扩展\n方差样条分解是多维样条的扩展\n当潜在特征数量较大时，自动方法更可取，MARS（多元自适应回归样条）和MART（多重加法回归树）方法\n树方法也是样条方法的一种，这值得思考，我认为树方法主要是在基函数上有不同\n基函数：截断幂基、自然立方样条、B-样条、光滑样条（局部多项式表示，便于无穷维空间内积），高斯径向基（全局表示，但随距离衰减），小波基（严格局部且多分辨率）\n选择多项式样条，当你认为数据的底层行为是分段变化的，并且你想在不同区域使用不同的简单模型时。 选择高斯RBF，当你需要拟合一个整体上非常平滑但高度非线性的函数，并且数据点之间的距离是关键信息时。 选择小波基，当你的数据（或函数）在不同位置表现出不同尺度的特征时，尤其是包含大量平滑区域和少数尖锐变化（如信号处理、图像压缩和金融时间序列分析）的场景。小波能够自适应地在需要的地方提供更高的分辨率，这是其无与伦比的优势。\n!!! tips 小波对图像的分解，是否能用于多分辨率扩散去噪模型？ 有待探索\n","date":"2025-06-13T16:56:05+08:00","permalink":"http://localhost:1313/hugo_dev/p/esl%E7%AC%AC%E4%BA%94%E7%AB%A0%E5%9F%BA%E5%B1%95%E5%BC%80%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/","title":"ESL第五章基展开和正则化"},{"content":"梯度下降对于线性可分数据和线性不可分数据的隐性偏置描述是不同的\n定理 9.3.1 对于几乎所有的线性可分数据集，采用梯度下降法进行更新，其中初始化权重 $w_0$ 和步长的选择都旨在最小化指数损失： $$ L(w) = \\sum_{i=1}^{n} \\exp\\left(-y^{(i)} w^T x^{(i)}\\right) $$ 即，$L(w_t) \\to 0$。梯度下降法的迭代过程最终会收敛至 $l_2$ 最大间隔向量的方向，即 $\\lim_{t \\to \\infty} \\frac{w_t}{||w_t||_2} = \\frac{\\hat{w}}{||\\hat{w}||_2}$，此处的 $\\hat{w}$ 为： $$ \\hat{w} = \\underset{w}{\\arg\\min} \\, ||w||_2^2 \\quad \\text{s.t.} \\quad \\forall i, \\, y^{(i)} w^T x^{(i)} \\ge 1 $$ 不失一般性，假设 $\\forall i, y^{(i)} = 1$ 作为线性模型的符号，因为该符号可以被合并到 $x^{(i)}$ 中。\n","date":"2025-06-12T12:03:02+08:00","permalink":"http://localhost:1313/hugo_dev/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%AC%94%E8%AE%B0/","title":"深度学习理论笔记"},{"content":"Hugo的下载与安装 下载Hugo网址：gohugo.io\n进入下载页面后，点击Github页面的tag，选择v0.131.0版本 创建新的站点：hugo new site [site-name] 创建新的博客文章：hugo new content post/\u0026lt;FOLDERNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\nHugo的配置 Stack 主题配置 hugo主题配置主要修改hugo.yaml这个文件，下面介绍一下这个文件中的各个参数。\n文件hugo.yaml参数说明 baseurl 指站点网址，配置完Github仓库后可以修改\npaginate 指一页几篇文章\ncopyright 版权所有，改成自己即可\nDefaultContentLanguage 站点默认语言，支持很多种选择zh-cn即可\nhasCJKLanguage 如果默认语言是中文zh-cn，那要改成True\nfavicon 用于修改标签页的小图标\nemoji 网上找一个emoji复制粘贴即可\navatar 是个人头像，网上搜一个放在对应文件夹下： ..\\assets\\img\\avatar.png subtitle 是个头像下面的文字\ncomments 是评论系统，官方文档提供了很多评论系统的支持\n修改下面图片中的英文：\n..\\content\\page 中的文件夹内index.md中的title和图片中英文一一对应，修改即可\nUtterances搭建评论系统 文章发布 每次新的文章发布后都要用这个代码刷新一下服务器：hugo server -D\nGithub部署 ","date":"2025-06-07T15:32:24+08:00","permalink":"http://localhost:1313/hugo_dev/p/hugo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","title":"Hugo个人博客搭建"},{"content":"亚马逊电商运营工作期间的一些总结 本人有幸参与过亚马逊电商运营的相关工作，其中主要涉及广告运营等内容。这部分与数据运营、计算广告、推荐系统有一定关系。希望从商家运营的角度出发做一些回顾与思考，希望对于推广搜有一定启发。先开个坑，日后慢慢填。 在运营方面的知识主要来源于上班期间上司的指导交流以及“知无不言”电商论坛。\n暂定内容计划：1. 亚马逊广告类型 2. 广告投放心得 3. 亚马逊平台的局限——独立站的重要性 4. 亚马逊平台的更新\n","date":"2025-04-30T10:40:07+08:00","permalink":"http://localhost:1313/hugo_dev/p/datamanagement/","title":"DataManagement"},{"content":"Hello, WOrld! ","date":"2025-04-30T00:17:45+08:00","permalink":"http://localhost:1313/hugo_dev/p/myfirstblog/","title":"MyFirstBlog"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"http://localhost:1313/hugo_dev/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu4699868770670889127.jpg","permalink":"http://localhost:1313/hugo_dev/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"http://localhost:1313/hugo_dev/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"http://localhost:1313/hugo_dev/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"http://localhost:1313/hugo_dev/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"http://localhost:1313/hugo_dev/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"http://localhost:1313/hugo_dev/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"http://localhost:1313/hugo_dev/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"http://localhost:1313/hugo_dev/p/emoji-support/","title":"Emoji Support"}]